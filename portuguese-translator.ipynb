{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:18.277708Z",
     "start_time": "2020-04-05T17:50:18.006907Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:21.367501Z",
     "start_time": "2020-04-05T17:50:18.279635Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:21.455479Z",
     "start_time": "2020-04-05T17:50:21.369479Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from : https://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:21.459480Z",
     "start_time": "2020-04-05T17:50:21.457480Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "#NUM_SAMPLES = 10000\n",
    "MAX_SEQ_LEN = 100\n",
    "#MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and Cleaning the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:23.883580Z",
     "start_time": "2020-04-05T17:50:23.828479Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = './por.txt'\n",
    "with open(filepath, encoding='UTF-8') as f:\n",
    "    eng2por = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:23.932481Z",
     "start_time": "2020-04-05T17:50:23.885479Z"
    }
   },
   "outputs": [],
   "source": [
    "eng2por = eng2por.split('\\n')[:-1] # Breaking in lines first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting English and Portuguese texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:24.058478Z",
     "start_time": "2020-04-05T17:50:23.934480Z"
    }
   },
   "outputs": [],
   "source": [
    "input_texts = [line.split('\\t')[0] for line in eng2por]\n",
    "translations = [line.split('\\t')[1] for line in eng2por]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs (English)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:29.066504Z",
     "start_time": "2020-04-05T17:50:25.899482Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_inputs = Tokenizer()\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting input word to index map and maximum size (for padding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:31.144926Z",
     "start_time": "2020-04-05T17:50:31.119932Z"
    }
   },
   "outputs": [],
   "source": [
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "max_len_input = max(len(s) for s in input_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding inputs (paddings are 'pre' by default) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:31.699024Z",
     "start_time": "2020-04-05T17:50:31.146924Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T17:17:50.229229Z",
     "start_time": "2020-04-04T17:17:50.223227Z"
    }
   },
   "source": [
    "Saving the input dictionary to calculate the embedding matrix (secondary script):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:31.704926Z",
     "start_time": "2020-04-05T17:50:31.700929Z"
    }
   },
   "outputs": [],
   "source": [
    "idx2word = {idx:word for (word, idx) in word2idx_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:31.717926Z",
     "start_time": "2020-04-05T17:50:31.706926Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./idx2word_encoder', 'wb') as f:\n",
    "    pickle.dump(idx2word, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translations (Portuguese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding tags to translations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:36.144609Z",
     "start_time": "2020-04-05T17:50:36.110640Z"
    }
   },
   "outputs": [],
   "source": [
    "translations = ['<sos> '+line+' <eos>' for line in translations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:38.775884Z",
     "start_time": "2020-04-05T17:50:36.146609Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_translations = Tokenizer(filters='')\n",
    "tokenizer_translations.fit_on_texts(translations)\n",
    "translations_sequences = tokenizer_translations.texts_to_sequences(translations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting translations dictionary, number of words and maximum target lentgh (for padding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:38.780788Z",
     "start_time": "2020-04-05T17:50:38.777791Z"
    }
   },
   "outputs": [],
   "source": [
    "word2idx_translations = tokenizer_translations.word_index\n",
    "num_words_output = len(word2idx_translations) + 1 # To account for 0 (padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating output and input for translations (Forced Teaching):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:38.974888Z",
     "start_time": "2020-04-05T17:50:38.782787Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_inputs = [sequence[:-1] for sequence in translations_sequences]\n",
    "trans_outputs = [sequence[1:] for sequence in translations_sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The targets are the trans_outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:39.008786Z",
     "start_time": "2020-04-05T17:50:38.976785Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len_target = max(len(s) for s in trans_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding data for the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T17:50:40.208898Z",
     "start_time": "2020-04-05T17:50:39.011789Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_inputs = pad_sequences(trans_inputs, padding='post',\n",
    "                               maxlen=max_len_target)\n",
    "targets = pad_sequences(trans_outputs, padding='post',\n",
    "                            maxlen=max_len_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do this because we can't one hot encode the whole target decoder sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T19:18:08.569926Z",
     "start_time": "2020-04-05T19:18:08.558895Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, encoder_inputs, decoder_inputs, targets,\n",
    "                 batch_size, num_words_output, shuffle=True):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_words_output = num_words_output\n",
    "        \n",
    "        # Data\n",
    "        self.encoder_inputs = encoder_inputs\n",
    "        self.decoder_inputs = decoder_inputs\n",
    "        self.targets = targets\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end() # Shuffle the dataset betweem epochs\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Number of batches per epoch\n",
    "        return int(self.encoder_inputs.shape[0] / self.batch_size)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        rows = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        enc_inp = self.encoder_inputs[rows, :]\n",
    "        dec_inp = self.decoder_inputs[rows, :]\n",
    "        dec_out_one_hot = to_categorical(self.targets[rows, :], \n",
    "                                         num_classes=self.num_words_output)\n",
    "        \n",
    "        return [enc_inp, dec_inp], dec_out_one_hot\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(self.encoder_inputs.shape[0])\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Model with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T19:17:36.039818Z",
     "start_time": "2020-04-05T19:17:36.037844Z"
    }
   },
   "outputs": [],
   "source": [
    "LATENT_DIM = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading embedding matrix and preparing embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T19:17:36.058818Z",
     "start_time": "2020-04-05T19:17:36.041818Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.load('./embedding_matrix_encoding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T19:17:36.065820Z",
     "start_time": "2020-04-05T19:17:36.060821Z"
    }
   },
   "outputs": [],
   "source": [
    "num_words = len(word2idx_inputs) + 1 # To account for padding\n",
    " \n",
    "embedding_layer = Embedding(\n",
    "    num_words,\n",
    "    EMBEDDING_DIM,\n",
    "    weights = [embedding_matrix],\n",
    "    input_length = max_len_input\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T19:17:36.413851Z",
     "start_time": "2020-04-05T19:17:36.067826Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_inputs_placeholder = Input(shape=(max_len_input, ))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(LATENT_DIM, return_state=True, dropout=0.5)\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "# We only need the final encoder states\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T19:17:36.797923Z",
     "start_time": "2020-04-05T19:17:36.419821Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input and embedding for decoder (not pre-trained in this case)\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
    "decoder_embedding = Embedding(num_words_output, LATENT_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "# Decoder LSTM\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_state=True, return_sequences=True,\n",
    "                    dropout=0.5)\n",
    "decoder_outputs, _, _ = decoder_lstm(\n",
    "    decoder_inputs_x,\n",
    "    initial_state = encoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T19:17:36.884816Z",
     "start_time": "2020-04-05T19:17:36.799820Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T19:17:36.889819Z",
     "start_time": "2020-04-05T19:17:36.886817Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs_placeholder, decoder_inputs_placeholder],\n",
    "              decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T19:17:36.925837Z",
     "start_time": "2020-04-05T19:17:36.891819Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T19:18:16.861979Z",
     "start_time": "2020-04-05T19:18:16.853974Z"
    }
   },
   "outputs": [],
   "source": [
    "data_generator = DataGenerator(encoder_inputs, decoder_inputs, targets,\n",
    "                               BATCH_SIZE, num_words_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-05T19:18:20.945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "   7/2468 [..............................] - ETA: 2:56:48 - loss: 8.0436 - accuracy: 0.6803"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=data_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
