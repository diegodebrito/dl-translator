{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:08:25.414682Z",
     "start_time": "2020-04-05T16:08:25.176603Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:08:28.458848Z",
     "start_time": "2020-04-05T16:08:25.416632Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:08:28.548849Z",
     "start_time": "2020-04-05T16:08:28.460815Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from : https://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:08:28.553817Z",
     "start_time": "2020-04-05T16:08:28.550817Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "#NUM_SAMPLES = 10000\n",
    "MAX_SEQ_LEN = 100\n",
    "#MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and Cleaning the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:08:28.608828Z",
     "start_time": "2020-04-05T16:08:28.555817Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = './por.txt'\n",
    "with open(filepath, encoding='UTF-8') as f:\n",
    "    eng2por = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:08:28.657845Z",
     "start_time": "2020-04-05T16:08:28.609815Z"
    }
   },
   "outputs": [],
   "source": [
    "eng2por = eng2por.split('\\n')[:-1] # Breaking in lines first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting English and Portuguese texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:08:28.774917Z",
     "start_time": "2020-04-05T16:08:28.659818Z"
    }
   },
   "outputs": [],
   "source": [
    "input_texts = [line.split('\\t')[0] for line in eng2por]\n",
    "translations = [line.split('\\t')[1] for line in eng2por]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs (English)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:08:31.987923Z",
     "start_time": "2020-04-05T16:08:28.777816Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_inputs = Tokenizer()\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting input word to index map and maximum size (for padding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:08:32.009900Z",
     "start_time": "2020-04-05T16:08:31.989817Z"
    }
   },
   "outputs": [],
   "source": [
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "max_len_input = max(len(s) for s in input_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding inputs (paddings are 'pre' by default) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:08:32.551920Z",
     "start_time": "2020-04-05T16:08:32.011816Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T17:17:50.229229Z",
     "start_time": "2020-04-04T17:17:50.223227Z"
    }
   },
   "source": [
    "Saving the input dictionary to calculate the embedding matrix (secondary script):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:08:32.557819Z",
     "start_time": "2020-04-05T16:08:32.553845Z"
    }
   },
   "outputs": [],
   "source": [
    "idx2word = {idx:word for (word, idx) in word2idx_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:08:32.571817Z",
     "start_time": "2020-04-05T16:08:32.559817Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./idx2word_encoder', 'wb') as f:\n",
    "    pickle.dump(idx2word, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translations (Portuguese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding tags to translations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:09:20.060629Z",
     "start_time": "2020-04-05T16:09:20.018638Z"
    }
   },
   "outputs": [],
   "source": [
    "translations = ['<sos> '+line+' <eos>' for line in translations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:09:22.396734Z",
     "start_time": "2020-04-05T16:09:20.061632Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_translations = Tokenizer(filters='')\n",
    "tokenizer_translations.fit_on_texts(translations)\n",
    "translations_sequences = tokenizer_translations.texts_to_sequences(translations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting translations dictionary, number of words and maximum target lentgh (for padding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:09:22.402630Z",
     "start_time": "2020-04-05T16:09:22.398632Z"
    }
   },
   "outputs": [],
   "source": [
    "word2idx_translations = tokenizer_translations.word_index\n",
    "num_words_output = len(word2idx_translations) + 1 # To account for 0 (padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating output and input for translations (Forced Teaching):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:09:22.683725Z",
     "start_time": "2020-04-05T16:09:22.404631Z"
    }
   },
   "outputs": [],
   "source": [
    "trans_inputs = [sequence[:-1] for sequence in translations_sequences]\n",
    "trans_outputs = [sequence[1:] for sequence in translations_sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The targets are the trans_outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:09:22.702627Z",
     "start_time": "2020-04-05T16:09:22.685652Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len_target = max(len(s) for s in trans_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding data for the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:09:23.719707Z",
     "start_time": "2020-04-05T16:09:22.703627Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_inputs = pad_sequences(trans_inputs, padding='post',\n",
    "                               maxlen=max_len_target)\n",
    "targets = pad_sequences(trans_outputs, padding='post',\n",
    "                            maxlen=max_len_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do this because we can't one hot encode the whole target decoder sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:09:57.541631Z",
     "start_time": "2020-04-05T16:09:57.536631Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data(enc_inp, dec_inp, target,\n",
    "                  num_words_output,\n",
    "                  batch_size=BATCH_SIZE):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        rows = np.random.randint(0, enc_inp.shape[0], BATCH_SIZE)\n",
    "        \n",
    "        enc_inp = enc_inp[rows, :]\n",
    "        dec_inp = dec_inp[rows, :]\n",
    "        dec_out_one_hot = to_categorical(target[rows, :], \n",
    "                                         num_classes=num_words_output)\n",
    "        \n",
    "        yield([enc_inp,\n",
    "               dec_inp],\n",
    "               dec_out_one_hot)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Model with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:10:28.792629Z",
     "start_time": "2020-04-05T16:10:28.789639Z"
    }
   },
   "outputs": [],
   "source": [
    "LATENT_DIM = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading embedding matrix and preparing embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:10:28.807632Z",
     "start_time": "2020-04-05T16:10:28.794632Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.load('./embedding_matrix_encoding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:10:28.814627Z",
     "start_time": "2020-04-05T16:10:28.809632Z"
    }
   },
   "outputs": [],
   "source": [
    "num_words = len(word2idx_inputs) + 1 # To account for padding\n",
    " \n",
    "embedding_layer = Embedding(\n",
    "    num_words,\n",
    "    EMBEDDING_DIM,\n",
    "    weights = [embedding_matrix],\n",
    "    input_length = max_len_input\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:10:29.183734Z",
     "start_time": "2020-04-05T16:10:28.815629Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_inputs_placeholder = Input(shape=(max_len_input, ))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(LATENT_DIM, return_state=True, dropout=0.5)\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "# We only need the final encoder states\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:10:29.647720Z",
     "start_time": "2020-04-05T16:10:29.184631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input and embedding for decoder (not pre-trained in this case)\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
    "decoder_embedding = Embedding(num_words_output, LATENT_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "# Decoder LSTM\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_state=True, return_sequences=True,\n",
    "                    dropout=0.5)\n",
    "decoder_outputs, _, _ = decoder_lstm(\n",
    "    decoder_inputs_x,\n",
    "    initial_state = encoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:10:29.743634Z",
     "start_time": "2020-04-05T16:10:29.649636Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:11:17.931677Z",
     "start_time": "2020-04-05T16:11:17.927643Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs_placeholder, decoder_inputs_placeholder],\n",
    "              decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:11:23.923675Z",
     "start_time": "2020-04-05T16:11:23.890642Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:14:30.481736Z",
     "start_time": "2020-04-05T16:14:30.477707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468.359375"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs.shape[0]/BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T16:14:45.652092Z",
     "start_time": "2020-04-05T16:14:45.646099Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-38-177ccf230077>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-38-177ccf230077>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    steps_per_epoch = 2468,\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generate_data(encoder_inputs,\n",
    "                                  decoder_inputs, \n",
    "                                  targets,\n",
    "                                  num_words_output,\n",
    "                                  batch_size=BATCH_SIZE)\n",
    "                   steps_per_epoch = 2468,\n",
    "                   epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
